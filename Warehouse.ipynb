{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium numpy matplotlib stable-baselines3[extra] tensorflow --quiet"
      ],
      "metadata": {
        "id": "cphjAvKiHZsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "C7Qu4f6HGfqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WarehouseEnvGeneralized(gym.Env):\n",
        "    \"\"\"\n",
        "    Gymnasium environment for optimizing warehouse layout.\n",
        "    (Docstrings like this explain the class's purpose)\n",
        "    \"\"\"\n",
        "    # Optional: Provides metadata, like supported rendering modes.\n",
        "    metadata = {'render_modes': ['human'], 'render_fps': 4}\n",
        "\n",
        "    # --- Initialization (`__init__`) ---\n",
        "    # This method runs ONCE when you create an instance of the environment (e.g., env = WarehouseEnvGeneralized())\n",
        "    def __init__(self, num_items=10, render_mode=None):\n",
        "        # Calls the initializer of the parent class (gym.Env) - necessary setup.\n",
        "        super().__init__()\n",
        "        # --- Environment Parameters ---\n",
        "        self.num_items = num_items # Store the number of items/positions in the warehouse\n",
        "        self.render_mode = render_mode # Store how rendering should happen (e.g., 'human' for printing)\n",
        "\n",
        "        # --- Action Space Definition ---\n",
        "        # Calculate the total number of unique pairs of items we can swap.\n",
        "        # Formula for combinations: n * (n-1) / 2\n",
        "        self._num_actions = self.num_items * (self.num_items - 1) // 2\n",
        "        # Create a list to easily map a discrete action index (0, 1, 2, ...) to the actual pair of indices to swap.\n",
        "        self._action_to_pair = []\n",
        "        for i in range(self.num_items):\n",
        "            for j in range(i + 1, self.num_items): # Ensure j > i to get unique pairs (swap (1,2) is same as (2,1))\n",
        "                self._action_to_pair.append((i, j))\n",
        "\n",
        "        # Define the action space using gymnasium.spaces.\n",
        "        # `spaces.Discrete` means the agent chooses one integer action from 0 up to (self._num_actions - 1).\n",
        "        self.action_space = spaces.Discrete(self._num_actions)\n",
        "\n",
        "        # --- Observation Space Definition ---\n",
        "        # Define what the agent \"sees\". We use `spaces.Dict` because the observation has multiple parts.\n",
        "        self.observation_space = spaces.Dict({\n",
        "            # 'layout': Represents the order of item IDs. It's an array of length `num_items`.\n",
        "            # Each element is an integer between 0 and `num_items - 1` (representing item IDs).\n",
        "            # `spaces.Box` defines a continuous or multi-dimensional discrete space.\n",
        "            'layout': spaces.Box(low=0, high=self.num_items - 1, shape=(self.num_items,), dtype=np.int32),\n",
        "\n",
        "            # 'demands': Represents the demand frequency for each ITEM ID (item 0, item 1, etc.).\n",
        "            # It's an array of length `num_items`. Demands range from 1 to 100.\n",
        "            'demands': spaces.Box(low=1, high=100, shape=(self.num_items,), dtype=np.int32)\n",
        "        })\n",
        "\n",
        "        # --- Internal State Variables ---\n",
        "        # These hold the current state of the environment. They are initialized in `reset`.\n",
        "        self.layout = None # Will hold the current array representing item positions (e.g., [3, 0, 1, ...])\n",
        "        self.demand_frequencies = None # Will hold the demand array for the current episode (e.g., [50, 88, ...])\n",
        "\n",
        "        # --- Rendering Variables (Optional) ---\n",
        "        # Used if you implement graphical rendering (e.g., with Pygame). Not used in the current text rendering.\n",
        "        self.window = None\n",
        "        self.clock = None\n",
        "\n",
        "    # --- Helper Method: `_get_obs` ---\n",
        "    # Creates the observation dictionary from the current internal state.\n",
        "    # Ensures the agent gets a *copy*, so modifying the observation doesn't change the internal state.\n",
        "    def _get_obs(self):\n",
        "        return {'layout': self.layout.copy(), 'demands': self.demand_frequencies.copy()}\n",
        "\n",
        "    # --- Helper Method: `_get_info` ---\n",
        "    # Returns extra information about the environment state, not used for training but useful for debugging/logging.\n",
        "    # Here, we calculate the current cost.\n",
        "    def _get_info(self):\n",
        "        # Cost = Sum over positions p: (p+1) * demand_of_item_at_position_p\n",
        "        cost = sum((pos + 1) * self.demand_frequencies[item_id] for pos, item_id in enumerate(self.layout))\n",
        "        return {'cost': cost}\n",
        "\n",
        "    # --- Core RL Method: `reset` ---\n",
        "    # Called at the beginning of every new episode. Prepares the environment for the next run.\n",
        "    def reset(self, seed=None, options=None):\n",
        "        # Crucial for reproducibility: seeds the environment's random number generator.\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        # --- Initialize State ---\n",
        "        # Set the layout to the default order [0, 1, 2, ..., num_items-1].\n",
        "        self.layout = np.arange(self.num_items, dtype=np.int32)\n",
        "\n",
        "        # *** KEY FOR GENERALIZATION ***\n",
        "        # Generate NEW random demand frequencies FOR EACH EPISODE.\n",
        "        # Uses the environment's seeded random number generator (`self.np_random`) for reproducibility.\n",
        "        # `integers(low, high, size)` generates integers >= low and < high.\n",
        "        self.demand_frequencies = self.np_random.integers(1, 101, size=self.num_items, dtype=np.int32)\n",
        "\n",
        "        # --- Prepare Return Values ---\n",
        "        # Get the initial observation based on the just-reset state.\n",
        "        observation = self._get_obs()\n",
        "        # Get any initial diagnostic info.\n",
        "        info = self._get_info()\n",
        "\n",
        "        # If rendering is enabled, show the initial state.\n",
        "        if self.render_mode == \"human\": self._render_frame()\n",
        "\n",
        "        # Standard Gymnasium `reset` returns the initial observation and info dictionary.\n",
        "        return observation, info\n",
        "\n",
        "    # --- Core RL Method: `step` ---\n",
        "    # This is where the main RL interaction happens. Takes an action, updates the state, calculates reward.\n",
        "    def step(self, action):\n",
        "        # --- Decode Action ---\n",
        "        # Convert the integer action chosen by the agent into the pair of layout indices to swap.\n",
        "        idx1, idx2 = self._action_to_pair[action]\n",
        "\n",
        "        # --- Update State (Environment Dynamics) ---\n",
        "        # Perform the swap on the current layout. This is the state transition.\n",
        "        self.layout[idx1], self.layout[idx2] = self.layout[idx2], self.layout[idx1]\n",
        "\n",
        "        # --- Calculate Reward ---\n",
        "        # The core signal guiding the agent's learning.\n",
        "        # Calculate the cost based on the NEW layout and the CURRENT episode's demands.\n",
        "        cost = sum((pos + 1) * self.demand_frequencies[item_id] for pos, item_id in enumerate(self.layout))\n",
        "        # **CRITICAL:** Reward is the NEGATIVE cost. RL agents maximize reward, so we want to maximize (-cost), which is equivalent to minimizing cost.\n",
        "        reward = -float(cost)\n",
        "\n",
        "        # --- Determine Episode End Conditions ---\n",
        "        # `terminated`: True if the episode ends naturally based on the environment's goal (e.g., reaching a target). Our env doesn't have this.\n",
        "        terminated = False\n",
        "        # `truncated`: True if the episode ends due to an external limit (e.g., time limit). We haven't added one here.\n",
        "        truncated = False\n",
        "\n",
        "        # --- Prepare Return Values ---\n",
        "        # Get the observation corresponding to the NEW state.\n",
        "        observation = self._get_obs()\n",
        "        # Get diagnostic info for the NEW state.\n",
        "        info = self._get_info()\n",
        "\n",
        "        # If rendering is enabled, show the state after the step.\n",
        "        if self.render_mode == \"human\": self._render_frame()\n",
        "\n",
        "        # Standard Gymnasium `step` returns 5 values: observation, reward, terminated, truncated, info\n",
        "        return observation, reward, terminated, truncated, info\n",
        "\n",
        "    # --- Optional Method: `render` ---\n",
        "    # Used to visualize the environment's state.\n",
        "    def render(self):\n",
        "         if self.render_mode == \"human\":\n",
        "             self._render_frame() # Calls the helper method\n",
        "\n",
        "    # --- Helper Method: `_render_frame` ---\n",
        "    # Contains the actual logic for rendering (in this case, printing to console).\n",
        "    def _render_frame(self):\n",
        "        if self.render_mode == \"human\":\n",
        "            # Print the current layout, demands for this episode, and the calculated cost.\n",
        "            print(f\"Layout: {self.layout}, Demands: {self.demand_frequencies}, Cost: {self._get_info()['cost']}\")\n",
        "\n",
        "    # --- Optional Method: `close` ---\n",
        "    # Called when the environment is no longer needed, to clean up resources (e.g., close rendering windows).\n",
        "    def close(self):\n",
        "        if self.window is not None: # Example for Pygame window cleanup\n",
        "            import pygame\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()\n"
      ],
      "metadata": {
        "id": "xGWkqxfvINHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    NUM_ITEMS = 10\n",
        "    TOTAL_TIMESTEPS = 1_000_000\n",
        "    MODEL_FILENAME = \"ppo_warehouse_generalized_v2.zip\"\n",
        "    STATS_FILENAME = \"vecnormalize_generalized_v2.pkl\"\n",
        "    LOG_DIR = \"./ppo_warehouse_gen_logs_v2/\"\n",
        "    os.makedirs(LOG_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "O3V18AuVV9XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_env(): # Helper function needed by DummyVecEnv\n",
        "        env = WarehouseEnvGeneralized(num_items=NUM_ITEMS)\n",
        "        return env\n",
        "\n",
        "vec_env = DummyVecEnv([make_env]) # Wrap the environment creator\n",
        "\n",
        "print(\"Wrapping environment with VecNormalize for reward scaling.\")\n",
        "vec_env = VecNormalize(vec_env, norm_obs=False, norm_reward=True, gamma=0.99) # Apply normalization"
      ],
      "metadata": {
        "id": "Qozrk4RMWEPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # --- Agent Training ---\n",
        "print(f\"\\n--- Training PPO with MultiInputPolicy for {TOTAL_TIMESTEPS} timesteps ---\")\n",
        "start_time = time.time() # Record start time\n",
        "\n",
        "# Instantiate the PPO Agent.\n",
        "model = PPO(\n",
        "    \"MultiInputPolicy\", # **IMPORTANT:** Use this policy because our observation space is a `spaces.Dict`. \"MlpPolicy\" is for flat `spaces.Box`.\n",
        "    vec_env,              # Train on the vectorized AND normalized environment.\n",
        "    verbose=1,            # Print training progress updates (0=none, 1=updates, 2=debug).\n",
        "    tensorboard_log=LOG_DIR, # Tell SB3 where to save logs for TensorBoard visualization.\n",
        "    # --- Hyperparameters (can be tuned for better performance) ---\n",
        "    learning_rate=1e-4,   # How big are the steps during policy updates (gradient descent). Smaller can be more stable but slower.\n",
        "    n_steps=2048,         # Number of steps collected from the env per agent update cycle. Larger batch -> more stable gradients.\n",
        "    batch_size=64,        # Size of minibatches used within each update epoch.\n",
        "    n_epochs=10,          # Number of times the collected data (`n_steps`) is iterated over during an update.\n",
        "    gamma=0.99,           # Discount factor for future rewards. Closer to 1 means more emphasis on long-term rewards. MUST MATCH VecNormalize gamma.\n",
        "    gae_lambda=0.95,      # Factor for Generalized Advantage Estimation (helps balance bias/variance in advantage calculation).\n",
        "    clip_range=0.2,       # PPO's clipping parameter to limit policy changes per update.\n",
        "    ent_coef=0.01,        # Entropy coefficient. Encourages exploration by adding a bonus for taking less predictable actions. Higher -> more exploration.\n",
        "    # policy_kwargs=dict(net_arch=...) # Can customize the neural network size here if needed.\n",
        ")\n",
        "\n",
        "# **THE ACTUAL TRAINING LOOP:** The agent interacts with `vec_env` for `TOTAL_TIMESTEPS` steps, updating its policy along the way.\n",
        "model.learn(total_timesteps=TOTAL_TIMESTEPS, progress_bar=True) # `progress_bar=True` shows a nice TQDM bar.\n",
        "end_time = time.time()\n",
        "print(f\"Training complete. Time taken: {end_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "jHU8cl2NWI72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Saving the trained model...\")\n",
        "model.save(MODEL_FILENAME)\n",
        "print(f\"Model saved to {MODEL_FILENAME}\")\n",
        "\n",
        "print(\"Saving VecNormalize statistics...\")\n",
        "# Important: Save the VecNormalize wrapper, not the base environment\n",
        "vec_env.save(STATS_FILENAME)\n",
        "print(f\"Normalization statistics saved to {STATS_FILENAME}\")\n",
        "\n",
        "# Close the training environment (releases resources)\n",
        "vec_env.close()\n",
        "print(\"Training environment closed.\")"
      ],
      "metadata": {
        "id": "FOAFuSqmjZVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n--- Evaluating the trained model ---\")\n",
        "\n",
        "# --- Parameters ---\n",
        "N_EVAL_EPISODES = 50  # How many different demand scenarios to test on\n",
        "EVAL_STEPS_PER_EPISODE = 200 # How many optimization steps the agent gets per scenario\n",
        "SEED = 42 # Use a fixed seed for evaluation for reproducibility\n",
        "\n",
        "# --- Load Model and Environment ---\n",
        "if not os.path.exists(MODEL_FILENAME) or not os.path.exists(STATS_FILENAME):\n",
        "    print(\"Error: Model or normalization statistics file not found.\")\n",
        "    print(\"Please train the model first (run the script without modification).\")\n",
        "    exit()\n",
        "\n",
        "print(f\"Loading model from {MODEL_FILENAME}\")\n",
        "model = PPO.load(MODEL_FILENAME)\n",
        "\n",
        "print(f\"Loading normalization statistics from {STATS_FILENAME}\")\n",
        "# Create a *new* environment instance for evaluation\n",
        "eval_env_raw = WarehouseEnvGeneralized(num_items=NUM_ITEMS) # Use the same num_items\n",
        "# Wrap it in DummyVecEnv\n",
        "eval_vec_env = DummyVecEnv([lambda: eval_env_raw])\n",
        "# Load the saved VecNormalize statistics\n",
        "eval_vec_env = VecNormalize.load(STATS_FILENAME, eval_vec_env)\n",
        "\n",
        "# **IMPORTANT:** Set VecNormalize to evaluation mode\n",
        "# This stops it from updating the running statistics (means/variances)\n",
        "eval_vec_env.training = False\n",
        "# This stops it from normalizing rewards (we want to see the true cost)\n",
        "eval_vec_env.norm_reward = False\n",
        "\n",
        "print(\"Evaluation environment ready.\")\n",
        "\n",
        "# --- Evaluation Loop ---\n",
        "initial_costs = []\n",
        "final_costs = []\n",
        "optimal_costs = []\n",
        "\n",
        "for i in range(N_EVAL_EPISODES):\n",
        "    print(f\"\\n--- Evaluation Episode {i+1}/{N_EVAL_EPISODES} ---\")\n",
        "    # Reset the environment - this generates NEW random demands\n",
        "    # We pass a specific seed to ensure reproducibility across evaluation runs *if needed*,\n",
        "    # but different seeds for each episode to test generalization.\n",
        "    # Using i as part of the seed ensures each episode gets different demands.\n",
        "    obs = eval_vec_env.reset() # Seed for reset is handled internally by VecEnv usually\n",
        "\n",
        "    # Get the underlying env to access its state directly\n",
        "    current_env = eval_vec_env.envs[0]\n",
        "    initial_demands = current_env.demand_frequencies.copy()\n",
        "    initial_layout = current_env.layout.copy()\n",
        "\n",
        "    # Calculate initial cost (before the agent acts)\n",
        "    # Note: We use the *unnormalized* reward/cost here.\n",
        "    initial_cost = sum((pos + 1) * initial_demands[item_id] for pos, item_id in enumerate(initial_layout))\n",
        "    initial_costs.append(initial_cost)\n",
        "    print(f\"Episode {i+1}: Initial Demands: {initial_demands}\")\n",
        "    print(f\"Episode {i+1}: Initial Layout: {initial_layout}\")\n",
        "    print(f\"Episode {i+1}: Initial Cost: {initial_cost}\")\n",
        "\n",
        "    # Calculate the theoretical optimal cost for this demand scenario\n",
        "    # Sort item IDs by demand (highest demand first)\n",
        "    sorted_item_ids_by_demand = np.argsort(initial_demands)[::-1]\n",
        "    optimal_cost = sum((pos + 1) * initial_demands[item_id] for pos, item_id in enumerate(sorted_item_ids_by_demand))\n",
        "    optimal_costs.append(optimal_cost)\n",
        "    print(f\"Episode {i+1}: Optimal Possible Cost: {optimal_cost}\")\n",
        "\n",
        "    # Let the agent optimize the layout for a fixed number of steps\n",
        "    current_cost = initial_cost\n",
        "    # Set render_mode if you want to see the steps (can be slow)\n",
        "    # current_env.render_mode = \"human\" # Uncomment to watch optimization\n",
        "\n",
        "    for step in range(EVAL_STEPS_PER_EPISODE):\n",
        "        # Get action from the loaded policy (deterministic=True means no random exploration)\n",
        "        action, _states = model.predict(obs, deterministic=True)\n",
        "        obs, reward, done, info = eval_vec_env.step(action)\n",
        "\n",
        "        # Get the true cost from the info dict (safer than using normalized reward)\n",
        "        # info is a list (one element per env in VecEnv), so access info[0]\n",
        "        current_cost = info[0]['cost']\n",
        "\n",
        "        # Optional: Render frame if needed\n",
        "        # if current_env.render_mode == \"human\":\n",
        "        #     current_env.render()\n",
        "        #     time.sleep(0.1) # Slow down rendering\n",
        "\n",
        "        # Environment doesn't terminate/truncate naturally in this setup\n",
        "        if done[0]:\n",
        "            print(\"Warning: Episode ended unexpectedly during evaluation step.\")\n",
        "            break # Should not happen with current env logic\n",
        "\n",
        "    # Record the final cost achieved by the agent\n",
        "    final_costs.append(current_cost)\n",
        "    print(f\"Episode {i+1}: Final Layout: {current_env.layout}\")\n",
        "    print(f\"Episode {i+1}: Final Cost after {EVAL_STEPS_PER_EPISODE} steps: {current_cost}\")\n",
        "    # Reset render mode if it was turned on\n",
        "    # current_env.render_mode = None\n",
        "\n",
        "# Close the evaluation environment\n",
        "eval_vec_env.close()\n",
        "\n",
        "# --- Report Results ---\n",
        "print(\"\\n\\n--- Evaluation Summary ---\")\n",
        "initial_costs = np.array(initial_costs)\n",
        "final_costs = np.array(final_costs)\n",
        "optimal_costs = np.array(optimal_costs)\n",
        "\n",
        "avg_initial_cost = np.mean(initial_costs)\n",
        "avg_final_cost = np.mean(final_costs)\n",
        "avg_optimal_cost = np.mean(optimal_costs)\n",
        "avg_improvement = avg_initial_cost - avg_final_cost\n",
        "avg_improvement_percent = (avg_improvement / avg_initial_cost) * 100 if avg_initial_cost > 0 else 0\n",
        "avg_gap_to_optimal = avg_final_cost - avg_optimal_cost\n",
        "avg_initial_gap_to_optimal = avg_initial_cost - avg_optimal_cost\n",
        "avg_percent_of_optimal_gap_closed = ((avg_initial_gap_to_optimal - avg_gap_to_optimal) / avg_initial_gap_to_optimal) * 100 if avg_initial_gap_to_optimal > 0 else float('inf')\n",
        "\n",
        "\n",
        "print(f\"Evaluated on {N_EVAL_EPISODES} episodes.\")\n",
        "print(f\"Average Initial Cost (random layout): {avg_initial_cost:.2f}\")\n",
        "print(f\"Average Final Cost (after agent optimization): {avg_final_cost:.2f}\")\n",
        "print(f\"Average Optimal Possible Cost: {avg_optimal_cost:.2f}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"Average Cost Reduction per Episode: {avg_improvement:.2f}\")\n",
        "print(f\"Average Cost Reduction Percentage: {avg_improvement_percent:.2f}%\")\n",
        "print(f\"Average Final Gap to Optimal Cost: {avg_gap_to_optimal:.2f}\")\n",
        "print(f\"Average Percentage of Optimal Gap Closed: {avg_percent_of_optimal_gap_closed:.2f}%\")\n",
        "\n",
        "# --- Optional Plotting ---\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(initial_costs, bins=15, alpha=0.7, label='Initial Costs')\n",
        "plt.hist(final_costs, bins=15, alpha=0.7, label='Final Costs (Agent)')\n",
        "plt.hist(optimal_costs, bins=15, alpha=0.7, label='Optimal Costs')\n",
        "plt.xlabel(\"Cost\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Costs across Episodes\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "episodes = range(1, N_EVAL_EPISODES + 1)\n",
        "plt.plot(episodes, initial_costs, 'o-', label='Initial Cost', alpha=0.6)\n",
        "plt.plot(episodes, final_costs, 's-', label='Final Cost (Agent)', alpha=0.6)\n",
        "plt.plot(episodes, optimal_costs, '^-', label='Optimal Cost', alpha=0.6)\n",
        "plt.xlabel(\"Evaluation Episode\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.title(\"Cost per Episode\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"evaluation_results_v2.png\") # Save the plot\n",
        "print(\"\\nSaved evaluation plot to evaluation_results_v2.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2_6UZt1YW6qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time\n",
        "from scipy import stats # Needed for Kendall's Tau\n",
        "\n",
        "# --- Assuming these are defined from the previous part ---\n",
        "# from stable_baselines3 import PPO\n",
        "# from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "# from your_environment_file import WarehouseEnvGeneralized # Import your env class\n",
        "# NUM_ITEMS = 10\n",
        "# MODEL_FILENAME = \"ppo_warehouse_generalized_v2.zip\"\n",
        "# STATS_FILENAME = \"vecnormalize_generalized_v2.pkl\"\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# --- Evaluation Focused on Layout Structure ---\n",
        "print(f\"\\n--- Evaluating Model based on High-Frequency Item Placement ---\")\n",
        "\n",
        "# --- Parameters ---\n",
        "N_EVAL_EPISODES = 50  # How many different demand scenarios to test on\n",
        "EVAL_STEPS_PER_EPISODE = 200 # How many optimization steps the agent gets per scenario\n",
        "TOP_K_VALUES = [1, 3, 5] # Evaluate placement accuracy for top 1, 3, and 5 items\n",
        "\n",
        "# --- Load Model and Environment ---\n",
        "if not os.path.exists(MODEL_FILENAME) or not os.path.exists(STATS_FILENAME):\n",
        "    print(\"Error: Model or normalization statistics file not found.\")\n",
        "    print(\"Please train the model first.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"Loading model from {MODEL_FILENAME}\")\n",
        "model = PPO.load(MODEL_FILENAME)\n",
        "\n",
        "print(f\"Loading normalization statistics from {STATS_FILENAME}\")\n",
        "eval_env_raw = WarehouseEnvGeneralized(num_items=NUM_ITEMS)\n",
        "eval_vec_env = DummyVecEnv([lambda: eval_env_raw])\n",
        "eval_vec_env = VecNormalize.load(STATS_FILENAME, eval_vec_env)\n",
        "eval_vec_env.training = False\n",
        "eval_vec_env.norm_reward = False\n",
        "print(\"Evaluation environment ready.\")\n",
        "\n",
        "# --- Evaluation Loop ---\n",
        "kendall_taus = []\n",
        "top_k_matches = {k: [] for k in TOP_K_VALUES} # {1: [matches_ep1, matches_ep2,...], 3: [...], ...}\n",
        "final_costs_agent = []\n",
        "optimal_costs_ideal = []\n",
        "\n",
        "\n",
        "for i in range(N_EVAL_EPISODES):\n",
        "    print(f\"\\n--- Structure Evaluation Episode {i+1}/{N_EVAL_EPISODES} ---\")\n",
        "    obs = eval_vec_env.reset()\n",
        "    current_env = eval_vec_env.envs[0] # Get the underlying environment instance\n",
        "    episode_demands = current_env.demand_frequencies.copy()\n",
        "    print(f\"Episode {i+1}: Demands: {episode_demands}\")\n",
        "\n",
        "    # 1. Calculate the IDEAL/OPTIMAL layout based on demands\n",
        "    # Argsort gives indices that would sort the array. [::-1] reverses for descending order.\n",
        "    # optimal_item_order contains the item IDs sorted by demand (highest demand first)\n",
        "    optimal_item_order = np.argsort(episode_demands)[::-1]\n",
        "    # This is the layout array representing the optimal placement\n",
        "    optimal_layout = optimal_item_order.astype(np.int32)\n",
        "    optimal_cost = sum((pos + 1) * episode_demands[item_id] for pos, item_id in enumerate(optimal_layout))\n",
        "    optimal_costs_ideal.append(optimal_cost)\n",
        "    print(f\"Episode {i+1}: Optimal Layout (by demand): {optimal_layout}\")\n",
        "    print(f\"Episode {i+1}: Optimal Cost: {optimal_cost:.2f}\")\n",
        "\n",
        "\n",
        "    # 2. Let the AGENT run and find its final layout\n",
        "    agent_final_cost = -1 # Initialize\n",
        "    for step in range(EVAL_STEPS_PER_EPISODE):\n",
        "        action, _states = model.predict(obs, deterministic=True)\n",
        "        obs, reward, done, info = eval_vec_env.step(action)\n",
        "        agent_final_cost = info[0]['cost'] # Get true cost from info\n",
        "        if done[0]: break # Should not happen here\n",
        "\n",
        "    agent_final_layout = current_env.layout.copy()\n",
        "    final_costs_agent.append(agent_final_cost)\n",
        "    print(f\"Episode {i+1}: Agent Final Layout: {agent_final_layout}\")\n",
        "    print(f\"Episode {i+1}: Agent Final Cost: {agent_final_cost:.2f}\")\n",
        "\n",
        "    # 3. Compare Agent's layout to the Optimal layout\n",
        "\n",
        "    # Metric 1: Kendall's Tau Rank Correlation\n",
        "    # Measures the similarity of the ordering of items between the two layouts.\n",
        "    # Ranges from -1 (perfect inverse) to +1 (perfect match). 0 means no correlation.\n",
        "    tau, p_value = stats.kendalltau(agent_final_layout, optimal_layout)\n",
        "    kendall_taus.append(tau)\n",
        "    print(f\"Episode {i+1}: Kendall's Tau: {tau:.4f}\")\n",
        "\n",
        "    # Metric 2: Top-K Placement Accuracy\n",
        "    # Checks if the highest-demand items are placed in the first K slots by the agent.\n",
        "    for k in TOP_K_VALUES:\n",
        "        # Set of item IDs that *should* be in the first K positions\n",
        "        ideal_top_k_items = set(optimal_layout[:k])\n",
        "        # Set of item IDs the agent *actually* placed in the first K positions\n",
        "        agent_top_k_items = set(agent_final_layout[:k])\n",
        "\n",
        "        # Count how many items are correctly placed within the top K\n",
        "        matches = len(ideal_top_k_items.intersection(agent_top_k_items))\n",
        "        top_k_matches[k].append(matches)\n",
        "        print(f\"Episode {i+1}: Top-{k} Matches: {matches}/{k}\")\n",
        "\n",
        "# Close the evaluation environment\n",
        "eval_vec_env.close()\n",
        "\n",
        "# --- Report Structure Evaluation Results ---\n",
        "print(\"\\n\\n--- Structure Evaluation Summary ---\")\n",
        "avg_kendall_tau = np.mean(kendall_taus)\n",
        "avg_final_cost_agent = np.mean(final_costs_agent)\n",
        "avg_optimal_cost_ideal = np.mean(optimal_costs_ideal)\n",
        "avg_cost_difference = avg_final_cost_agent - avg_optimal_cost_ideal\n",
        "\n",
        "print(f\"Evaluated on {N_EVAL_EPISODES} episodes with {EVAL_STEPS_PER_EPISODE} agent steps each.\")\n",
        "print(f\"Average Agent Final Cost: {avg_final_cost_agent:.2f}\")\n",
        "print(f\"Average Optimal (Demand-Sorted) Cost: {avg_optimal_cost_ideal:.2f}\")\n",
        "print(f\"Average Cost Difference (Agent - Optimal): {avg_cost_difference:.2f}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"Average Kendall's Tau Rank Correlation: {avg_kendall_tau:.4f}\")\n",
        "print(\" (Closer to 1.0 means the agent's layout order is more similar to the optimal demand-based order)\")\n",
        "print(\"-\" * 30)\n",
        "print(\"Average Top-K Item Placement Accuracy:\")\n",
        "for k in TOP_K_VALUES:\n",
        "    avg_matches = np.mean(top_k_matches[k])\n",
        "    percent_accuracy = (avg_matches / k) * 100\n",
        "    print(f\"  - Top-{k}: Agent placed an average of {avg_matches:.2f} / {k} correct items ({percent_accuracy:.1f}%)\")\n",
        "\n",
        "# --- Optional Plotting for Structure Evaluation ---\n",
        "plt.figure(figsize=(18, 6))\n",
        "\n",
        "# Plot 1: Histogram of Kendall's Tau\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.hist(kendall_taus, bins=10, alpha=0.7, color='skyblue')\n",
        "plt.axvline(avg_kendall_tau, color='red', linestyle='dashed', linewidth=1, label=f'Avg: {avg_kendall_tau:.3f}')\n",
        "plt.xlabel(\"Kendall's Tau\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Layout Rank Correlation (Agent vs Optimal)\")\n",
        "plt.xlim([-1.1, 1.1]) # Kendall's Tau range\n",
        "plt.legend()\n",
        "plt.grid(axis='y', linestyle='--')\n",
        "\n",
        "# Plot 2: Bar chart for Top-K Accuracy\n",
        "plt.subplot(1, 3, 2)\n",
        "k_labels = [f'Top-{k}' for k in TOP_K_VALUES]\n",
        "avg_matches_values = [np.mean(top_k_matches[k]) for k in TOP_K_VALUES]\n",
        "bars = plt.bar(k_labels, avg_matches_values, color='lightgreen')\n",
        "plt.ylabel(\"Average Number of Correct Items\")\n",
        "plt.title(\"Average Top-K Item Placement Accuracy\")\n",
        "# Add percentage labels on bars\n",
        "for bar, k, avg_val in zip(bars, TOP_K_VALUES, avg_matches_values):\n",
        "    percent = (avg_val / k) * 100\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{percent:.1f}%', ha='center', va='bottom')\n",
        "plt.ylim(0, max(TOP_K_VALUES) * 1.1) # Adjust y-limit based on max K\n",
        "plt.grid(axis='y', linestyle='--')\n",
        "\n",
        "# Plot 3: Agent Cost vs Optimal Cost Scatter Plot\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.scatter(optimal_costs_ideal, final_costs_agent, alpha=0.6, label='Episode Result')\n",
        "# Add line y=x for reference (perfect agent)\n",
        "max_cost = max(max(optimal_costs_ideal), max(final_costs_agent))\n",
        "min_cost = min(min(optimal_costs_ideal), min(final_costs_agent))\n",
        "plt.plot([min_cost, max_cost], [min_cost, max_cost], 'r--', label='Optimal Cost Line (y=x)')\n",
        "plt.xlabel(\"Optimal (Demand-Sorted) Cost\")\n",
        "plt.ylabel(\"Agent's Final Cost\")\n",
        "plt.title(\"Agent Cost vs. Theoretical Optimal Cost\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"evaluation_structure_results_v2.png\")\n",
        "print(\"\\nSaved structure evaluation plot to evaluation_structure_results_v2.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_FTC2wVfjRIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fv2KJgY2PCKZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}